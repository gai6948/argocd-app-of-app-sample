apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: loki
spec:
  project: default
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - ApplyOutOfSyncOnly=true
  destination:
    name: in-cluster
    namespace: loki
  source:
    repoURL: "https://grafana.github.io/helm-charts"
    chart: loki
    targetRevision: "2.11.0"
    helm:
      values: |
        ingress:
          enabled: false

        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - loki
            topologyKey: "kubernetes.io/hostname"

        config:
          # existingSecret:
          auth_enabled: false
          ingester:
            max_chunk_age: 2h
            chunk_encoding: snappy
            chunk_idle_period: 1h
            chunk_block_size: 262144
            chunk_retain_period: 1m
            max_transfer_retries: 0
            wal:
              enabled: true
              dir: /data/loki/wal
            lifecycler:
              ring:
                kvstore:
                  store: inmemory
                replication_factor: 1

          limits_config:
            enforce_metric_name: false
            reject_old_samples: true
            reject_old_samples_max_age: 168h
          schema_config:
            configs:
            - from: 2020-10-24
              store: boltdb-shipper
              object_store: filesystem
              schema: v11
              index:
                prefix: index_
                period: 24h
          server:
            http_listen_port: 3100
          storage_config:
            boltdb_shipper:
              active_index_directory: /data/loki/boltdb-shipper-active
              cache_location: /data/loki/boltdb-shipper-cache
              cache_ttl: 24h         # Can be increased for faster performance over longer query periods, uses more disk space
              shared_store: filesystem
            filesystem:
              directory: /data/loki/chunks
          chunk_store_config:
            max_look_back_period: 0s
          table_manager:
            retention_deletes_enabled: true
            retention_period: 168h
          compactor:
            working_directory: /data/loki/boltdb-shipper-compactor
            shared_store: filesystem
        # Needed for Alerting: https://grafana.com/docs/loki/latest/alerting/
        # This is just a simple example, for more details: https://grafana.com/docs/loki/latest/configuration/#ruler_config
          ruler:
            storage:
              type: local
              local:
                directory: /rules
            rule_path: /tmp/scratch
            alertmanager_url: http://kube-prometheus-kube-prome-alertmanager:9093
            ring:
              kvstore:
                store: inmemory
            enable_api: true
          analytics:
            reporting_enabled: false

        nodeSelector:
          kubernetes.io/arch: arm64

        persistence:
          enabled: true
          accessModes:
          - ReadWriteOnce
          size: 50Gi
          storageClassName: openebs-lvmpv

        podAnnotations:
          prometheus.io/scrape: "true"
          prometheus.io/port: "http-metrics"
          io.cilium.proxy-visibility: "<Egress/53/UDP/DNS>,<Ingress/3100/TCP/HTTP>"

        rbac:
          create: true
          pspEnabled: false

        replicas: 1

        resources:
          limits:
            cpu: 2000m
            memory: 2048Mi
          requests:
            cpu: 500m
            memory: 512Mi

        serviceMonitor:
          enabled: true

        # Specify Loki Alerting rules based on this documentation: https://grafana.com/docs/loki/latest/alerting/
        # When specified, you also need to add a ruler config section above. An example is shown in the alerting docs.
        alerting_groups:
          - name: default
            rules:
            - alert: HighThroughputLogStreams
              expr: sum by(job) (rate({job=~".+"}[1m])) > 100
              for: 2m
